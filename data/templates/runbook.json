[
  {
    "instruction": "Write a runbook for handling this alert",
    "input": "Alert: High CPU usage on web server (>90% for 5 minutes)\nServer: web-prod-01\nService: nginx + application",
    "output": "# Runbook: High CPU Alert - Web Server\n\n**Alert:** CPU usage > 90% sustained for 5+ minutes\n**Severity:** Warning → Critical if > 15 minutes\n**On-Call Response:** 10 minutes\n\n---\n\n## Quick Reference\n\n```\nServer: web-prod-01\nServices: nginx, app-server\nLogs: /var/log/nginx/, /var/log/app/\nDashboard: grafana.internal/d/webserver\n```\n\n## Step 1: Initial Assessment (2 min)\n\n```bash\n# SSH to server\nssh admin@web-prod-01\n\n# Check current CPU usage and top processes\ntop -bn1 | head -20\n\n# Check load average\nuptime\n\n# Quick process summary\nps aux --sort=-%cpu | head -10\n```\n\n**Decision Point:**\n- If single process using high CPU → Go to Step 2a\n- If many processes → Go to Step 2b\n- If system processes (kswapd, etc.) → Go to Step 2c\n\n## Step 2a: Single Process High CPU\n\n```bash\n# Identify the process\nps aux --sort=-%cpu | head -5\n\n# Check if it's stuck\nstrace -p <PID> -c\n\n# For nginx workers\nnginx -t\nsystemctl status nginx\n\n# For app server\nsystemctl status app-server\njournalctl -u app-server -n 100\n```\n\n**Resolution Options:**\n1. Restart the stuck process: `systemctl restart <service>`\n2. Kill runaway process: `kill -9 <PID>` (last resort)\n3. Scale horizontally if legitimate load\n\n## Step 2b: High Overall Load\n\n```bash\n# Check request rate\ntail -f /var/log/nginx/access.log | pv -l -i10 > /dev/null\n\n# Check for attack patterns\nawk '{print $1}' /var/log/nginx/access.log | sort | uniq -c | sort -rn | head -20\n\n# Check active connections\nnetstat -an | grep :443 | wc -l\n```\n\n**Resolution Options:**\n1. Enable rate limiting in nginx\n2. Block offending IPs: `iptables -A INPUT -s <IP> -j DROP`\n3. Remove from load balancer temporarily\n4. Scale up additional servers\n\n## Step 2c: System Process Issues\n\n```bash\n# Check memory pressure (causes kswapd)\nfree -h\ncat /proc/meminfo | grep -i swap\n\n# Check disk I/O\niostat -x 1 5\n\n# Check for OOM events\ndmesg | grep -i \"out of memory\"\n```\n\n**Resolution Options:**\n1. Add swap space temporarily\n2. Restart memory-heavy applications\n3. Increase server memory (requires maintenance window)\n\n## Step 3: Verify Resolution\n\n```bash\n# Confirm CPU normalized\nwatch -n1 'uptime; echo; top -bn1 | head -5'\n\n# Check service health\ncurl -I http://localhost/health\n\n# Monitor for 5 minutes before closing\n```\n\n## Escalation\n\n- If unresolved after 15 minutes → Page senior engineer\n- If customer impact confirmed → Notify incident commander\n- If security concern → Engage security team\n\n## Post-Incident\n\n- [ ] Update monitoring thresholds if needed\n- [ ] Document root cause in incident ticket\n- [ ] Schedule capacity review if load-related"
  },
  {
    "instruction": "Write a runbook for handling this alert",
    "input": "Alert: Database connection pool exhausted\nService: PostgreSQL\nApplication: Customer API\nThreshold: 0 available connections",
    "output": "# Runbook: Database Connection Pool Exhausted\n\n**Alert:** Connection pool at 0 available connections\n**Severity:** Critical (immediate customer impact)\n**On-Call Response:** 5 minutes\n\n---\n\n## Quick Reference\n\n```\nDatabase: postgres-prod-01\nApplication: customer-api\nPool Size: 100 connections (default)\nDashboard: grafana.internal/d/postgres\n```\n\n## Step 1: Immediate Triage (1 min)\n\n```bash\n# Check database connectivity\npsql -h postgres-prod-01 -U monitor -c \"SELECT 1;\"\n\n# View active connections\npsql -h postgres-prod-01 -U admin -c \"\nSELECT count(*), state, application_name \nFROM pg_stat_activity \nGROUP BY state, application_name \nORDER BY count DESC;\"\n```\n\n**Expected Output Analysis:**\n- `active` - queries running (should be < 20)\n- `idle` - waiting in pool (normal)\n- `idle in transaction` - ⚠️ PROBLEM - leaked connections\n\n## Step 2: Identify Connection Leaks\n\n```sql\n-- Find long-running queries\nSELECT pid, now() - pg_stat_activity.query_start AS duration, query, state\nFROM pg_stat_activity\nWHERE (now() - pg_stat_activity.query_start) > interval '5 minutes'\nAND state != 'idle'\nORDER BY duration DESC;\n\n-- Find idle in transaction (leaked connections)\nSELECT pid, now() - pg_stat_activity.xact_start AS duration, query\nFROM pg_stat_activity\nWHERE state = 'idle in transaction'\nAND xact_start < now() - interval '1 minute'\nORDER BY duration DESC;\n```\n\n## Step 3: Resolution Actions\n\n### Option A: Kill Leaked Connections (Safest)\n\n```sql\n-- Kill idle in transaction connections older than 5 min\nSELECT pg_terminate_backend(pid)\nFROM pg_stat_activity\nWHERE state = 'idle in transaction'\nAND xact_start < now() - interval '5 minutes';\n```\n\n### Option B: Kill Long-Running Queries\n\n```sql\n-- Kill queries running > 10 minutes (careful!)\nSELECT pg_terminate_backend(pid)\nFROM pg_stat_activity\nWHERE state = 'active'\nAND query_start < now() - interval '10 minutes'\nAND query NOT LIKE '%pg_stat_activity%';\n```\n\n### Option C: Restart Application Pool\n\n```bash\n# Restart customer-api to reset connection pool\nkubectl rollout restart deployment/customer-api -n production\n\n# Or for VM-based\nsystemctl restart customer-api\n```\n\n### Option D: Increase Pool Size (Temporary)\n\n```sql\n-- Increase max connections (requires careful consideration)\nALTER SYSTEM SET max_connections = 150;\nSELECT pg_reload_conf();\n```\n\n## Step 4: Verify Resolution\n\n```bash\n# Check connection count trending down\nwatch -n5 'psql -h postgres-prod-01 -U monitor -c \"\nSELECT count(*) as total, \n       count(*) FILTER (WHERE state = '\\''active'\\'') as active,\n       count(*) FILTER (WHERE state = '\\''idle'\\'') as idle\nFROM pg_stat_activity;\"'\n\n# Verify application health\ncurl -s http://customer-api/health | jq .\n```\n\n## Root Cause Investigation\n\nCommon causes:\n1. **Missing connection release** - Code bug not closing connections\n2. **Transaction timeout** - Long transactions holding connections\n3. **Traffic spike** - Legitimate increased load\n4. **Slow queries** - Queries taking too long\n\n```bash\n# Check recent deployments\nkubectl rollout history deployment/customer-api -n production\n\n# Check traffic patterns\ncurl -s \"http://prometheus:9090/api/v1/query?query=rate(http_requests_total[5m])\"\n```\n\n## Escalation\n\n- If connections don't decrease after killing → Page DBA\n- If application keeps leaking after restart → Page dev team\n- If database unresponsive → Page infrastructure\n\n## Prevention Checklist\n\n- [ ] Implement connection timeout in app config\n- [ ] Add idle_in_transaction_session_timeout to PostgreSQL\n- [ ] Review connection pool settings vs max_connections\n- [ ] Add alerting at 80% pool usage"
  }
]
